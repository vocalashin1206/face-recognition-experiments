{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== Training Code for Four Models (with Two Comparison Plots) ======\nimport os\nimport random\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\n\n# ================= Configuration =================\nclass Config:\n    \"\"\"\n    Configuration parameters\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    MS1M_PATH = \"/kaggle/input/ms1m-arcface-dataset/ms1m-arcface\"\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 15\n    LEARNING_RATE = 0.01\n    FEATURE_DIM = 512\n    SCALE_FACTOR = 30.0\n    BASE_MARGIN = 0.5\n    ALPHA = 0.3\n    MODES = [\"fixed_margin\", \"quality_adaptive\", \"confidence_adaptive\", \"easy_hard_norm\"]\n\nconfig = Config()\nprint(f\"Using device: {config.device}\")\n\n# ================= Dataset =================\nclass FaceDataset(Dataset):\n    \"\"\"\n    Custom face dataset\n    1. Load a specified number of identities, each with a certain number of images\n    2. Returns (image_tensor, label)\n    \"\"\"\n    def __init__(self, root_dir, num_identities=200, samples_per_identity=15, transform=None):\n        self.transform = transform\n        self.samples = []\n        \n        all_folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n        selected_identities = random.sample(all_folders, min(num_identities, len(all_folders)))\n        \n        for identity in selected_identities:\n            identity_path = os.path.join(root_dir, identity)\n            image_files = [f for f in os.listdir(identity_path) if f.lower().endswith(('.jpg','.png','.jpeg'))]\n            \n            if len(image_files) < 5:\n                continue\n                \n            selected_images = random.sample(image_files, min(samples_per_identity, len(image_files)))\n            for img_file in selected_images:\n                self.samples.append((os.path.join(identity_path, img_file), identity))\n        \n        unique_identities = list(set([identity for _,identity in self.samples]))\n        self.identity_to_label = {identity: idx for idx, identity in enumerate(unique_identities)}\n        self.num_classes = len(unique_identities)\n        \n        print(f\"Loaded {len(self.samples)} images, {self.num_classes} identities\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, identity = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.identity_to_label[identity]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# ================= ArcFace Model =================\nclass ArcFaceModel(nn.Module):\n    \"\"\"\n    ResNet18 + ArcFace classification\n    1. Supports per-sample margin\n    2. Supports direct extraction of backbone features (used for margin calculation)\n    \"\"\"\n    def __init__(self, num_classes):\n        super().__init__()\n        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.feature_extractor = nn.Sequential(*list(self.backbone.children())[:-1])\n        self.feature_dim = 512\n        self.fc = nn.Linear(self.feature_dim, num_classes, bias=False)\n        nn.init.normal_(self.fc.weight, std=0.01)\n        self.dropout = nn.Dropout(0.2)\n\n    def get_features(self, x):\n        \"\"\"\n        Safely extract raw features (without dropout)\n        \"\"\"\n        feat = self.feature_extractor(x)\n        feat = feat.view(feat.size(0), -1)\n        return feat\n\n    def forward(self, x, margin=None):\n        \"\"\"\n        Forward pass\n        margin: None / scalar / tensor(B,) for per-sample margin\n        \"\"\"\n        feat = self.feature_extractor(x)\n        feat = feat.view(feat.size(0), -1)\n        feat = self.dropout(feat)\n        feat_norm = F.normalize(feat, p=2, dim=1)\n\n        weight_norm = F.normalize(self.fc.weight, p=2, dim=1)\n\n        cosine = torch.matmul(feat_norm, weight_norm.t())\n\n        # per-sample margin support\n        if margin is None:\n            margin_tensor = 0.0\n        else:\n            if isinstance(margin, (float, int)):\n                margin_tensor = float(margin)\n            elif isinstance(margin, torch.Tensor):\n                if margin.dim() == 1 and margin.size(0) == cosine.size(0):\n                    margin_tensor = margin.view(-1, 1).to(cosine.device)\n                else:\n                    margin_tensor = float(margin.mean().item())\n            else:\n                margin_tensor = float(margin)\n\n        cosine = cosine - margin_tensor\n        output = cosine * config.SCALE_FACTOR\n        return feat, output\n\n# ================= Margin Calculation =================\ndef calculate_margin(mode, features=None, images=None, logits=None, device=None):\n    \"\"\"\n    Returns a per-sample margin tensor of shape (B,)\n    1. fixed_margin: all samples share the same margin\n    2. quality_adaptive: based on image sharpness\n    3. confidence_adaptive: based on softmax top-1 probability / feature norm\n    4. easy_hard_norm: based on feature norm\n    \"\"\"\n    device = device if device is not None else config.device\n    B = features.size(0)\n\n    if mode == \"fixed_margin\":\n        return torch.full((B,), config.BASE_MARGIN, device=device, dtype=torch.float32)\n\n    elif mode == \"quality_adaptive\":\n        margins = []\n        images_cpu = images.detach().cpu()\n        for i in range(images_cpu.size(0)):\n            img_np = (images_cpu[i].permute(1,2,0).numpy() * 255).astype(np.uint8)\n            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n            normalized_sharpness = np.clip(sharpness / 1000.0, 0.0, 1.0)\n            margin = config.BASE_MARGIN * (1.0 + config.ALPHA * normalized_sharpness)\n            margins.append(margin)\n        margins = np.array(margins, dtype=np.float32)\n        return torch.from_numpy(margins).to(device)\n\n    elif mode == \"confidence_adaptive\":\n        # based on feature norm confidence\n        feat_norms = torch.norm(features, p=2, dim=1)\n        mean_norm = feat_norms.mean()\n        std_norm = feat_norms.std()\n        conf_norm = (feat_norms - mean_norm) / (std_norm + 1e-8)\n        conf_norm = torch.clamp(conf_norm, -1.0, 1.0)\n\n        # image quality scores (sampled subset)\n        quality_scores = []\n        images_cpu = images.detach().cpu()\n        for i in range(min(5, images_cpu.size(0))):\n            img_np = (images_cpu[i].permute(1,2,0).numpy() * 255).astype(np.uint8)\n            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n            quality_scores.append(min(sharpness / 1000.0, 1.0))\n        avg_quality = np.mean(quality_scores) if quality_scores else 0.5\n\n        combined_conf = 0.7 * conf_norm + 0.3 * avg_quality\n        margins = config.BASE_MARGIN * (0.8 + 0.2 * combined_conf)\n        return margins.to(device)\n\n    elif mode == \"easy_hard_norm\":\n        feat_norms = torch.norm(features, p=2, dim=1)\n        mean_norm = feat_norms.mean()\n        std_norm = feat_norms.std()\n        norm_scores = (feat_norms - mean_norm) / (std_norm + 1e-8)\n\n        margins = torch.empty_like(norm_scores)\n        margins[norm_scores >= 0] = config.BASE_MARGIN * (1 + config.ALPHA)  # easy samples\n        margins[norm_scores < 0] = config.BASE_MARGIN * (1 - config.ALPHA)   # hard samples\n        return margins.to(device)\n\n    return torch.full((B,), config.BASE_MARGIN, device=device, dtype=torch.float32)\n\n# ================= Training Function =================\ndef train_single_model(mode):\n    \"\"\"\n    Train a single model\n    Returns: train_loss_list, val_acc_list (recorded per epoch)\n    \"\"\"\n    print(f\"\\nðŸŽ¯ Start training {mode} model...\")\n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.RandomCrop((112, 112)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n\n    dataset = FaceDataset(config.MS1M_PATH, num_identities=200, samples_per_identity=15, transform=transform)\n\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n\n    model = ArcFaceModel(num_classes=dataset.num_classes).to(config.device)\n    optimizer = optim.SGD(model.parameters(), lr=config.LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)\n\n    best_val_acc = 0.0\n    patience = 5\n    patience_counter = 0\n\n    # record curves\n    train_loss_list = []\n    val_acc_list = []\n\n    for epoch in range(config.NUM_EPOCHS):\n        # ===== Training =====\n        model.train()\n        epoch_loss = 0.0\n        train_correct, train_total = 0, 0\n\n        for imgs, labels in tqdm(train_loader, desc=f\"Training {mode}\"):\n            imgs, labels = imgs.to(config.device), labels.to(config.device)\n            # compute margin (no gradient)\n            with torch.no_grad():\n                features_for_margin = model.get_features(imgs)\n                _, logits_for_margin = model(imgs, margin=None)\n                margins = calculate_margin(mode, features=features_for_margin,\n                                           images=imgs, logits=logits_for_margin, device=config.device)\n\n            # forward + backward\n            features, outputs = model(imgs, margin=margins)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            _, predicted = torch.max(outputs.data, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n            epoch_loss += loss.item()\n\n        avg_train_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n        train_loss_list.append(avg_train_loss)\n        train_acc = train_correct / train_total if train_total > 0 else 0.0\n\n        # ===== Validation =====\n        model.eval()\n        val_correct, val_total = 0, 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(config.device), labels.to(config.device)\n                _, outputs = model(imgs, margin=None)\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n        val_acc = val_correct / val_total if val_total > 0 else 0.0\n        val_acc_list.append(val_acc)\n\n        scheduler.step()\n        print(f\"{mode} Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n\n        # early stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            os.makedirs('/kaggle/working/models', exist_ok=True)\n            torch.save(model.state_dict(), f'/kaggle/working/models/best_model_{mode}.pth')\n            print(f\"Saved best model: val_acc = {val_acc:.4f}\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    print(f\"{mode} model training completed, best val_acc: {best_val_acc:.4f}\")\n    return model, val_loader, best_val_acc, train_loss_list, val_acc_list\n\n# ================= Main Loop (Train Four Models) + Plot =================\nif __name__ == \"__main__\":\n    print(\"Start training four margin models...\")\n    results = {}\n    curve_loss = {}\n    curve_valacc = {}\n\n    for mode in config.MODES:\n        try:\n            model, val_loader, best_acc, train_loss_list, val_acc_list = train_single_model(mode)\n            results[mode] = best_acc\n            curve_loss[mode] = train_loss_list\n            curve_valacc[mode] = val_acc_list\n        except Exception as e:\n            print(f\"{mode} model training failed: {e}\")\n            continue\n\n    print(\"\\n All models training completed!\")\n    print(\"Final results:\")\n    for mode, acc in results.items():\n        print(f\"  {mode}: val_acc = {acc:.4f}\")\n\n    # ===== Plot setup (four models in one figure) =====\n    os.makedirs('/kaggle/working/plots', exist_ok=True)\n\n    # color dictionary: distinguishable colors\n    color_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n    colors = {mode: color_list[i % len(color_list)] for i, mode in enumerate(config.MODES)}\n\n    # ===== Figure 1: Train Loss =====\n    plt.figure(figsize=(10, 6))\n    max_epochs = max(len(v) for v in curve_loss.values()) if curve_loss else 0\n    for mode in config.MODES:\n        if mode in curve_loss:\n            epochs = range(1, len(curve_loss[mode]) + 1)\n            plt.plot(epochs, curve_loss[mode], label=mode, color=colors.get(mode), linewidth=1)\n    plt.title(\"Train Loss Comparison (All Models)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Train Loss\")\n    plt.xticks(range(1, max_epochs + 1))\n    plt.legend()\n    plt.grid(True)\n    train_loss_path = '/kaggle/working/plots/train_loss_comparison.pdf'\n    plt.savefig(train_loss_path, bbox_inches='tight')\n    plt.show()\n    print(f\"Saved: {train_loss_path}\")\n\n    # ===== Figure 2: Validation Accuracy =====\n    plt.figure(figsize=(10, 6))\n    max_epochs_val = max(len(v) for v in curve_valacc.values()) if curve_valacc else 0\n    for mode in config.MODES:\n        if mode in curve_valacc:\n            epochs = range(1, len(curve_valacc[mode]) + 1)\n            plt.plot(epochs, curve_valacc[mode], label=mode, color=colors.get(mode), linewidth=1)\n    plt.title(\"Validation Accuracy Comparison (All Models)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.xticks(range(1, max_epochs_val + 1))\n    plt.legend()\n    plt.grid(True, linestyle='-', color='lightgray', linewidth=0.8, alpha=0.7)\n    val_acc_path = '/kaggle/working/plots/val_acc_comparison.pdf'\n    plt.savefig(val_acc_path, bbox_inches='tight')\n    plt.show()\n    print(f\"Saved: {val_acc_path}\")\n\n    # List saved model files (if any)\n    if os.path.exists('/kaggle/working/models'):\n        saved_models = [f for f in os.listdir('/kaggle/working/models') if f.endswith('.pth')]\n    else:\n        saved_models = []\n    print(f\"Saved model files: {saved_models}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\n\n# ====== Test configuration ======\nclass TestConfig:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    LFW_PATH = \"/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled\"\n    BATCH_SIZE = 32\n    FEATURE_DIM = 512\n\ntest_config = TestConfig()\n\n# ====== ArcFace model definition consistent with training ======\nclass ArcFaceModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.feature_extractor = nn.Sequential(*list(self.backbone.children())[:-1])\n        self.feature_dim = 512\n        self.fc = nn.Linear(self.feature_dim, num_classes, bias=False)\n        nn.init.normal_(self.fc.weight, std=0.01)\n        self.dropout = nn.Dropout(0.2)\n\n    def get_features(self, x):\n        \"\"\"Safely obtain raw features (without dropout)\"\"\"\n        feat = self.feature_extractor(x)\n        feat = feat.view(feat.size(0), -1)\n        return feat\n\n    def forward(self, x, margin=None):\n        \"\"\"Forward pass identical to training code\"\"\"\n        feat = self.feature_extractor(x)\n        feat = feat.view(feat.size(0), -1)\n        feat = self.dropout(feat)  # used in training, disabled in eval()\n        feat_norm = F.normalize(feat, p=2, dim=1)\n\n        weight_norm = F.normalize(self.fc.weight, p=2, dim=1)\n        cosine = torch.matmul(feat_norm, weight_norm.t())\n\n        # per-sample margin support\n        if margin is None:\n            margin_tensor = 0.0\n        else:\n            if isinstance(margin, (float, int)):\n                margin_tensor = float(margin)\n            elif isinstance(margin, torch.Tensor):\n                if margin.dim() == 1 and margin.size(0) == cosine.size(0):\n                    margin_tensor = margin.view(-1, 1).to(cosine.device)\n                else:\n                    margin_tensor = float(margin.mean().item())\n            else:\n                margin_tensor = float(margin)\n\n        cosine = cosine - margin_tensor\n        output = cosine * 30.0  # use the scale factor from training\n        return feat, output\n\n# ====== LFW test dataset ======\nclass LFWDataset(Dataset):\n    def __init__(self, lfw_path, transform=None, num_pairs=600):\n        self.transform = transform\n        self.lfw_path = lfw_path\n        self.pairs = self._build_pairs_from_lfw(num_pairs)\n        \n    def _build_pairs_from_lfw(self, num_pairs):\n        \"\"\"Build pairs directly from the LFW dataset\"\"\"\n        print(f\"Building test pairs from LFW dataset: {self.lfw_path}\")\n        \n        if not os.path.exists(self.lfw_path):\n            print(f\"ERROR: LFW path does not exist: {self.lfw_path}\")\n            return []\n        \n        person_folders = [f for f in os.listdir(self.lfw_path) \n                         if os.path.isdir(os.path.join(self.lfw_path, f))]\n        \n        print(f\"Found {len(person_folders)} person folders\")\n        \n        if len(person_folders) == 0:\n            print(\"ERROR: No person folders found\")\n            return []\n        \n        person_images = {}\n        valid_persons = []\n        \n        for person in person_folders:\n            person_path = os.path.join(self.lfw_path, person)\n            images = [f for f in os.listdir(person_path) \n                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            if images:\n                person_images[person] = images\n                valid_persons.append(person)\n        \n        print(f\"Valid persons: {len(valid_persons)}\")\n        \n        if len(valid_persons) < 2:\n            print(\"ERROR: Not enough valid persons to build pairs\")\n            return []\n        \n        pairs = []\n        same_person_count = num_pairs // 2\n        diff_person_count = num_pairs - same_person_count\n        \n        same_count = 0\n        persons_with_multiple = [p for p in valid_persons if len(person_images[p]) >= 2]\n        \n        print(f\"Persons with multiple images: {len(persons_with_multiple)}\")\n        \n        for person in persons_with_multiple:\n            if same_count >= same_person_count:\n                break\n            images = person_images[person]\n            img1, img2 = random.sample(images, 2)\n            img1_path = os.path.join(self.lfw_path, person, img1)\n            img2_path = os.path.join(self.lfw_path, person, img2)\n            pairs.append((img1_path, img2_path, 1))\n            same_count += 1\n        \n        diff_count = 0\n        while diff_count < diff_person_count and len(valid_persons) >= 2:\n            person1, person2 = random.sample(valid_persons, 2)\n            images1 = person_images[person1]\n            images2 = person_images[person2]\n            \n            if images1 and images2:\n                img1 = random.choice(images1)\n                img2 = random.choice(images2)\n                img1_path = os.path.join(self.lfw_path, person1, img1)\n                img2_path = os.path.join(self.lfw_path, person2, img2)\n                pairs.append((img1_path, img2_path, 0))\n                diff_count += 1\n        \n        print(f\"Built {len(pairs)} test pairs (same: {same_count}, different: {diff_count})\")\n        \n        if pairs:\n            print(\"\\nExample pairs:\")\n            for i in range(min(3, len(pairs))):\n                path1, path2, label = pairs[i]\n                name1 = os.path.basename(os.path.dirname(path1))\n                name2 = os.path.basename(os.path.dirname(path2))\n                img1 = os.path.basename(path1)\n                img2 = os.path.basename(path2)\n                relation = \"same person\" if label == 1 else \"different person\"\n                print(f\"  {i+1}. {name1}/{img1} vs {name2}/{img2} - {relation}\")\n        \n        return pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img1_path, img2_path, label = self.pairs[idx]\n        \n        try:\n            img1 = Image.open(img1_path).convert('RGB')\n            img2 = Image.open(img2_path).convert('RGB')\n        except Exception as e:\n            print(f\"Failed to load image: {e}\")\n            img1 = Image.new('RGB', (112, 112), color='gray')\n            img2 = Image.new('RGB', (112, 112), color='gray')\n        \n        if self.transform:\n            img1 = self.transform(img1)\n            img2 = self.transform(img2)\n            \n        return img1, img2, torch.tensor(label, dtype=torch.float32)\n\n# ====== Improved model loading function ======\ndef load_trained_model(model_path):\n    \"\"\"Load a trained model - automatically handle class count mismatch\"\"\"\n    try:\n        # Load state dict first to detect class count\n        state_dict = torch.load(model_path, map_location=test_config.device)\n        \n        # Handle different storage formats\n        if isinstance(state_dict, dict):\n            # If dict, check if it includes 'state_dict' and other info\n            if 'state_dict' in state_dict:\n                # New format: contains state_dict and metadata\n                actual_state_dict = state_dict['state_dict']\n                if 'num_classes' in state_dict:\n                    num_classes = state_dict['num_classes']\n                    print(f\"Loaded number of classes from metadata: {num_classes}\")\n                else:\n                    # Infer num_classes from fc.weight\n                    num_classes = actual_state_dict['fc.weight'].shape[0]\n                    print(f\"Inferred number of classes from fc.weight: {num_classes}\")\n            else:\n                # Old format: only state dict\n                actual_state_dict = state_dict\n                num_classes = actual_state_dict['fc.weight'].shape[0]\n                print(f\"Inferred number of classes from fc.weight: {num_classes}\")\n        else:\n            # If it's directly a state dict\n            actual_state_dict = state_dict\n            num_classes = actual_state_dict['fc.weight'].shape[0]\n            print(f\"Inferred number of classes from fc.weight: {num_classes}\")\n        \n        # Create model\n        model = ArcFaceModel(num_classes=num_classes)\n        \n        # Simple key processing: remove 'module.' prefix\n        new_state_dict = {}\n        for k, v in actual_state_dict.items():\n            if k.startswith('module.'):\n                new_k = k[7:]\n            else:\n                new_k = k\n            new_state_dict[new_k] = v\n        \n        # Try strict loading\n        try:\n            model.load_state_dict(new_state_dict, strict=True)\n            print(\"Strict load succeeded\")\n        except RuntimeError as e:\n            if \"size mismatch\" in str(e):\n                print(\"Size mismatch detected, attempting flexible loading...\")\n                # Flexible loading: only load matching layers\n                model_state_dict = model.state_dict()\n                filtered_state_dict = {}\n                \n                for key, value in new_state_dict.items():\n                    if key in model_state_dict:\n                        if model_state_dict[key].shape == value.shape:\n                            filtered_state_dict[key] = value\n                        else:\n                            print(f\"  Skipping mismatched layer: {key} (expected: {model_state_dict[key].shape}, actual: {value.shape})\")\n                    else:\n                        print(f\"  Skipping non-existing layer: {key}\")\n                \n                # Load filtered state dict\n                model.load_state_dict(filtered_state_dict, strict=False)\n                print(f\"Flexible load complete, loaded {len(filtered_state_dict)}/{len(new_state_dict)} layers\")\n            else:\n                raise e\n        \n        model.to(test_config.device)\n        model.eval()  # important: disable dropout and set BN to eval\n        \n        # Verify model loaded correctly\n        with torch.no_grad():\n            test_input = torch.randn(1, 3, 112, 112).to(test_config.device)\n            features, outputs = model(test_input)\n            print(f\"Model verification: feature shape {features.shape}, output shape {outputs.shape}\")\n            \n        return model\n        \n    except Exception as e:\n        print(f\"Failed to load model {model_path}: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n# ====== Feature extraction ======\ndef extract_features(model, images):\n    \"\"\"Extract image features - use get_features for consistency\"\"\"\n    with torch.no_grad():\n        # Use get_features instead of forward to ensure features used for margin calculation are consistent\n        features = model.get_features(images)\n        features = F.normalize(features, p=2, dim=1)\n    return features\n\n# ====== Compute accuracy and ROC curve ======\ndef evaluate_model(model, test_loader, model_name):\n    \"\"\"Evaluate model performance on the test set\"\"\"\n    if len(test_loader.dataset) == 0:\n        print(f\"ERROR: Test dataset is empty, cannot evaluate {model_name}\")\n        return 0, 0, 0, [], []\n    \n    similarities = []\n    labels = []\n    \n    print(f\"Evaluating model {model_name}...\")\n    \n    with torch.no_grad():\n        for img1, img2, label in tqdm(test_loader, desc=f\"Testing {model_name}\"):\n            img1, img2 = img1.to(test_config.device), img2.to(test_config.device)\n            \n            # Use consistent feature extraction method\n            feat1 = extract_features(model, img1)\n            feat2 = extract_features(model, img2)\n            \n            similarity = F.cosine_similarity(feat1, feat2)\n            \n            similarities.extend(similarity.cpu().numpy())\n            labels.extend(label.numpy())\n    \n    similarities = np.array(similarities)\n    labels = np.array(labels)\n    \n    # Compute ROC and AUC\n    fpr, tpr, thresholds = roc_curve(labels, similarities)\n    roc_auc = auc(fpr, tpr)\n    \n    gmeans = np.sqrt(tpr * (1-fpr))\n    ix = np.argmax(gmeans)\n    best_threshold = thresholds[ix]\n    \n    predictions = (similarities >= best_threshold).astype(int)\n    accuracy = np.mean(predictions == labels)\n    \n    print(f\"{model_name} results:\")\n    print(f\"   Accuracy: {accuracy:.4f}\")\n    print(f\"   AUC: {roc_auc:.4f}\")\n    print(f\"   Best threshold: {best_threshold:.4f}\")\n    print(f\"   Similarity range: [{similarities.min():.3f}, {similarities.max():.3f}]\")\n    \n    return accuracy, roc_auc, best_threshold, fpr, tpr\n\n# ====== Plot ROC curves ======\ndef plot_roc_curves(results, save_path='/kaggle/working/roc_curves.pdf'):\n    \"\"\"Plot ROC curves for all models\"\"\"\n    plt.figure(figsize=(10, 8))\n    \n    colors = ['red', 'blue', 'green', 'orange']\n    for i, (model_name, (accuracy, roc_auc, _, fpr, tpr)) in enumerate(results.items()):\n        plt.plot(fpr, tpr, color=colors[i], lw=2, \n                label=f'{model_name} (AUC = {roc_auc:.3f}, Acc = {accuracy:.3f})')\n    \n\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('LFW - ROC')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    \n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n\n# ====== Analyze results ======\ndef analyze_results(results):\n    \"\"\"Analyze test results\"\"\"\n    print(\"\\nResults Analysis:\")\n    print(\"=\" * 30)\n    \n    if len(results) < 2:\n        print(\"At least two model results are required for comparison\")\n        return\n    \n    sorted_results = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n    \n    print(\"Model ranking (by accuracy):\")\n    for i, (model_name, (accuracy, roc_auc, threshold, _, _)) in enumerate(sorted_results):\n        print(f\"  {i+1}. {model_name}: {accuracy:.4f}\")\n    \n    best_acc = sorted_results[0][1][0]\n    worst_acc = sorted_results[-1][1][0]\n    improvement = best_acc - worst_acc\n    \n    print(f\"\\nBest model accuracy is higher than worst model by: {improvement:.4f} ({improvement*100:.2f}%)\")\n    \n    if \"fixed_margin\" in results:\n        fixed_acc = results[\"fixed_margin\"][0]\n        for model_name, (accuracy, _, _, _, _) in results.items():\n            if model_name != \"fixed_margin\":\n                diff = accuracy - fixed_acc\n                if diff > 0:\n                    print(f\"{model_name} is better than fixed_margin: +{diff:.4f} (+{diff*100:.2f}%)\")\n                elif diff < 0:\n                    print(f\"{model_name} is worse than fixed_margin: {diff:.4f} ({diff*100:.2f}%)\")\n                else:\n                    print(f\"{model_name} is equal to fixed_margin\")\n\n# ====== Main test function ======\ndef test_models_on_lfw():\n    \"\"\"Test all models on the LFW dataset\"\"\"\n    \n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.CenterCrop((112, 112)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    print(\"Creating LFW test dataset...\")\n    test_dataset = LFWDataset(\n        lfw_path=test_config.LFW_PATH,\n        transform=transform,\n        num_pairs=600\n    )\n    \n    if len(test_dataset) == 0:\n        print(\"ERROR: Failed to create LFW test dataset\")\n        return\n    \n    test_loader = DataLoader(test_dataset, batch_size=test_config.BATCH_SIZE, shuffle=False)\n    \n    model_paths = {\n        \"fixed_margin\": \"/kaggle/working/models/best_model_fixed_margin.pth\",\n        \"quality_adaptive\": \"/kaggle/working/models/best_model_quality_adaptive.pth\", \n        \"confidence_adaptive\": \"/kaggle/working/models/best_model_confidence_adaptive.pth\",\n        \"easy_hard_norm\": \"/kaggle/working/models/best_model_easy_hard_norm.pth\"\n    }\n\n    results = {}\n\n    for model_name, model_path in model_paths.items():\n        if not os.path.exists(model_path):\n           print(f\"WARNING: Model not found: {model_path}\")\n           continue\n\n        print(f\"\\n{'='*50}\")\n        print(f\"Loading and testing model: {model_name}...\")\n        print(f\"{'='*50}\")\n\n        model = load_trained_model(model_path)\n        if model is None:\n            continue\n\n        # LFW evaluation uses cosine similarity without training-time margin by default\n        accuracy, roc_auc, threshold, fpr, tpr = evaluate_model(model, test_loader, model_name)\n        results[model_name] = (accuracy, roc_auc, threshold, fpr, tpr)\n\n        if torch.cuda.is_available():\n           torch.cuda.empty_cache()\n\n    \n    if results:\n        plot_roc_curves(results)\n        \n        print(\"\\nLFW final results:\")\n        print(\"=\" * 50)\n        for model_name, (accuracy, roc_auc, threshold, _, _) in results.items():\n            print(f\"{model_name:20} | Accuracy: {accuracy:.4f} | AUC: {roc_auc:.4f} | Threshold: {threshold:.4f}\")\n        \n        best_model = max(results.items(), key=lambda x: x[1][0])\n        print(f\"\\nBest model: {best_model[0]} (Accuracy: {best_model[1][0]:.4f})\")\n        \n        analyze_results(results)\n        \n        with open('/kaggle/working/lfw_test_results.txt', 'w') as f:\n            f.write(\"LFW Test Results\\n\")\n            f.write(\"=\" * 50 + \"\\n\")\n            for model_name, (accuracy, roc_auc, threshold, _, _) in results.items():\n                f.write(f\"{model_name:20} | Accuracy: {accuracy:.4f} | AUC: {roc_auc:.4f} | Threshold: {threshold:.4f}\\n\")\n            f.write(f\"\\nBest model: {best_model[0]} (Accuracy: {best_model[1][0]:.4f})\\n\")\n        \n        print(\"Results saved to: /kaggle/working/lfw_test_results.txt\")\n    else:\n        print(\"ERROR: No models were successfully tested\")\n\n# ====== Run tests ======\nif __name__ == \"__main__\":\n    print(\"Starting LFW evaluation...\")\n    print(f\"Using LFW path: {test_config.LFW_PATH}\")\n    test_models_on_lfw()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}