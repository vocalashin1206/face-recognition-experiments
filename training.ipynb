{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== Training Code ======\nimport os\nimport random\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\n\n# ================= Configuration =================\nclass Config:\n    \"\"\"\n    Configuration parameters\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    MS1M_PATH = \"/kaggle/input/ms1m-arcface-dataset/ms1m-arcface\"\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 15\n    LEARNING_RATE = 0.01\n    FEATURE_DIM = 512\n    SCALE_FACTOR = 30.0\n    BASE_MARGIN = 0.5\n    ALPHA = 0.3\n    MODES = [\"fixed_margin\", \"quality_adaptive\", \"confidence_adaptive\", \"easy_hard_norm\"]\n\nconfig = Config()\nprint(f\"Using device: {config.device}\")\n\n# ================= Dataset =================\nclass FaceDataset(Dataset):\n    \"\"\"\n    Custom face dataset\n    1. Load a specified number of identities, each with a certain number of images\n    2. Returns (image_tensor, label)\n    \"\"\"\n    def __init__(self, root_dir, num_identities=200, samples_per_identity=15, transform=None):\n        self.transform = transform\n        self.samples = []\n        \n        all_folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n        selected_identities = random.sample(all_folders, min(num_identities, len(all_folders)))\n        \n        for identity in selected_identities:\n            identity_path = os.path.join(root_dir, identity)\n            image_files = [f for f in os.listdir(identity_path) if f.lower().endswith(('.jpg','.png','.jpeg'))]\n            \n            if len(image_files) < 5:\n                continue\n                \n            selected_images = random.sample(image_files, min(samples_per_identity, len(image_files)))\n            for img_file in selected_images:\n                self.samples.append((os.path.join(identity_path, img_file), identity))\n        \n        unique_identities = list(set([identity for _,identity in self.samples]))\n        self.identity_to_label = {identity: idx for idx, identity in enumerate(unique_identities)}\n        self.num_classes = len(unique_identities)\n        \n        print(f\"Loaded {len(self.samples)} images, {self.num_classes} identities\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, identity = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.identity_to_label[identity]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# ================= ArcFace Model =================\nclass ArcFaceModel(nn.Module):\n    \"\"\"\n    ResNet18 + ArcFace classification\n    1. Supports per-sample margin\n    2. Supports direct extraction of backbone features (used for margin calculation)\n    \"\"\"\n    def __init__(self, num_classes):\n        super().__init__()\n        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.feature_extractor = nn.Sequential(*list(self.backbone.children())[:-1])\n        self.feature_dim = 512\n        self.fc = nn.Linear(self.feature_dim, num_classes, bias=False)\n        nn.init.normal_(self.fc.weight, std=0.01)\n        self.dropout = nn.Dropout(0.2)\n\n    def get_features(self, x):\n        \"\"\"\n        Safely extract raw features (without dropout)\n        \"\"\"\n        feat = self.feature_extractor(x)\n        feat = feat.view(feat.size(0), -1)\n        return feat\n\n    def forward(self, x, margin=None):\n        \"\"\"\n        Forward pass\n        margin: None / scalar / tensor(B,) for per-sample margin\n        \"\"\"\n        feat = self.feature_extractor(x)\n        feat = feat.view(feat.size(0), -1)\n        feat = self.dropout(feat)\n        feat_norm = F.normalize(feat, p=2, dim=1)\n\n        weight_norm = F.normalize(self.fc.weight, p=2, dim=1)\n\n        cosine = torch.matmul(feat_norm, weight_norm.t())\n\n        # per-sample margin support\n        if margin is None:\n            margin_tensor = 0.0\n        else:\n            if isinstance(margin, (float, int)):\n                margin_tensor = float(margin)\n            elif isinstance(margin, torch.Tensor):\n                if margin.dim() == 1 and margin.size(0) == cosine.size(0):\n                    margin_tensor = margin.view(-1, 1).to(cosine.device)\n                else:\n                    margin_tensor = float(margin.mean().item())\n            else:\n                margin_tensor = float(margin)\n\n        cosine = cosine - margin_tensor\n        output = cosine * config.SCALE_FACTOR\n        return feat, output\n\n# ================= Margin Calculation =================\ndef calculate_margin(mode, features=None, images=None, logits=None, device=None):\n    \"\"\"\n    Returns a per-sample margin tensor of shape (B,)\n    1. fixed_margin: all samples share the same margin\n    2. quality_adaptive: based on image sharpness\n    3. confidence_adaptive: based on softmax top-1 probability / feature norm\n    4. easy_hard_norm: based on feature norm\n    \"\"\"\n    device = device if device is not None else config.device\n    B = features.size(0)\n\n    if mode == \"fixed_margin\":\n        return torch.full((B,), config.BASE_MARGIN, device=device, dtype=torch.float32)\n\n    elif mode == \"quality_adaptive\":\n        margins = []\n        images_cpu = images.detach().cpu()\n        for i in range(images_cpu.size(0)):\n            img_np = (images_cpu[i].permute(1,2,0).numpy() * 255).astype(np.uint8)\n            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n            normalized_sharpness = np.clip(sharpness / 1000.0, 0.0, 1.0)\n            margin = config.BASE_MARGIN * (1.0 + config.ALPHA * normalized_sharpness)\n            margins.append(margin)\n        margins = np.array(margins, dtype=np.float32)\n        return torch.from_numpy(margins).to(device)\n\n    elif mode == \"confidence_adaptive\":\n        # based on feature norm confidence\n        feat_norms = torch.norm(features, p=2, dim=1)\n        mean_norm = feat_norms.mean()\n        std_norm = feat_norms.std()\n        conf_norm = (feat_norms - mean_norm) / (std_norm + 1e-8)\n        conf_norm = torch.clamp(conf_norm, -1.0, 1.0)\n\n        # image quality scores (sampled subset)\n        quality_scores = []\n        images_cpu = images.detach().cpu()\n        for i in range(min(5, images_cpu.size(0))):\n            img_np = (images_cpu[i].permute(1,2,0).numpy() * 255).astype(np.uint8)\n            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n            quality_scores.append(min(sharpness / 1000.0, 1.0))\n        avg_quality = np.mean(quality_scores) if quality_scores else 0.5\n\n        combined_conf = 0.7 * conf_norm + 0.3 * avg_quality\n        margins = config.BASE_MARGIN * (0.8 + 0.2 * combined_conf)\n        return margins.to(device)\n\n    elif mode == \"easy_hard_norm\":\n        feat_norms = torch.norm(features, p=2, dim=1)\n        mean_norm = feat_norms.mean()\n        std_norm = feat_norms.std()\n        norm_scores = (feat_norms - mean_norm) / (std_norm + 1e-8)\n\n        margins = torch.empty_like(norm_scores)\n        margins[norm_scores >= 0] = config.BASE_MARGIN * (1 + config.ALPHA)  # easy samples\n        margins[norm_scores < 0] = config.BASE_MARGIN * (1 - config.ALPHA)   # hard samples\n        return margins.to(device)\n\n    return torch.full((B,), config.BASE_MARGIN, device=device, dtype=torch.float32)\n\n# ================= Training Function =================\ndef train_single_model(mode):\n    \"\"\"\n    Train a single model\n    Returns: train_loss_list, val_acc_list (recorded per epoch)\n    \"\"\"\n    print(f\"\\nðŸŽ¯ Start training {mode} model...\")\n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.RandomCrop((112, 112)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n\n    dataset = FaceDataset(config.MS1M_PATH, num_identities=200, samples_per_identity=15, transform=transform)\n\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n\n    model = ArcFaceModel(num_classes=dataset.num_classes).to(config.device)\n    optimizer = optim.SGD(model.parameters(), lr=config.LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)\n\n    best_val_acc = 0.0\n    patience = 5\n    patience_counter = 0\n\n    # record curves\n    train_loss_list = []\n    val_acc_list = []\n\n    for epoch in range(config.NUM_EPOCHS):\n        # ===== Training =====\n        model.train()\n        epoch_loss = 0.0\n        train_correct, train_total = 0, 0\n\n        for imgs, labels in tqdm(train_loader, desc=f\"Training {mode}\"):\n            imgs, labels = imgs.to(config.device), labels.to(config.device)\n            # compute margin (no gradient)\n            with torch.no_grad():\n                features_for_margin = model.get_features(imgs)\n                _, logits_for_margin = model(imgs, margin=None)\n                margins = calculate_margin(mode, features=features_for_margin,\n                                           images=imgs, logits=logits_for_margin, device=config.device)\n\n            # forward + backward\n            features, outputs = model(imgs, margin=margins)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            _, predicted = torch.max(outputs.data, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n            epoch_loss += loss.item()\n\n        avg_train_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n        train_loss_list.append(avg_train_loss)\n        train_acc = train_correct / train_total if train_total > 0 else 0.0\n\n        # ===== Validation =====\n        model.eval()\n        val_correct, val_total = 0, 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(config.device), labels.to(config.device)\n                _, outputs = model(imgs, margin=None)\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n        val_acc = val_correct / val_total if val_total > 0 else 0.0\n        val_acc_list.append(val_acc)\n\n        scheduler.step()\n        print(f\"{mode} Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n\n        # early stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            os.makedirs('/kaggle/working/models', exist_ok=True)\n            torch.save(model.state_dict(), f'/kaggle/working/models/best_model_{mode}.pth')\n            print(f\"Saved best model: val_acc = {val_acc:.4f}\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    print(f\"{mode} model training completed, best val_acc: {best_val_acc:.4f}\")\n    return model, val_loader, best_val_acc, train_loss_list, val_acc_list\n\n# ================= Main Loop (Train Four Models) + Plot =================\nif __name__ == \"__main__\":\n    print(\"Start training four margin models...\")\n    results = {}\n    curve_loss = {}\n    curve_valacc = {}\n\n    for mode in config.MODES:\n        try:\n            model, val_loader, best_acc, train_loss_list, val_acc_list = train_single_model(mode)\n            results[mode] = best_acc\n            curve_loss[mode] = train_loss_list\n            curve_valacc[mode] = val_acc_list\n        except Exception as e:\n            print(f\"{mode} model training failed: {e}\")\n            continue\n\n    print(\"\\n All models training completed!\")\n    print(\"Final results:\")\n    for mode, acc in results.items():\n        print(f\"  {mode}: val_acc = {acc:.4f}\")\n\n    # ===== Plot setup (four models in one figure) =====\n    os.makedirs('/kaggle/working/plots', exist_ok=True)\n\n    # color dictionary: distinguishable colors\n    color_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n    colors = {mode: color_list[i % len(color_list)] for i, mode in enumerate(config.MODES)}\n\n    # ===== Figure 1: Train Loss =====\n    plt.figure(figsize=(10, 6))\n    max_epochs = max(len(v) for v in curve_loss.values()) if curve_loss else 0\n    for mode in config.MODES:\n        if mode in curve_loss:\n            epochs = range(1, len(curve_loss[mode]) + 1)\n            plt.plot(epochs, curve_loss[mode], label=mode, color=colors.get(mode), linewidth=1)\n    plt.title(\"Train Loss Comparison (All Models)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Train Loss\")\n    plt.xticks(range(1, max_epochs + 1))\n    plt.legend()\n    plt.grid(True)\n    train_loss_path = '/kaggle/working/plots/train_loss_comparison.pdf'\n    plt.savefig(train_loss_path, bbox_inches='tight')\n    plt.show()\n    print(f\"Saved: {train_loss_path}\")\n\n    # ===== Figure 2: Validation Accuracy =====\n    plt.figure(figsize=(10, 6))\n    max_epochs_val = max(len(v) for v in curve_valacc.values()) if curve_valacc else 0\n    for mode in config.MODES:\n        if mode in curve_valacc:\n            epochs = range(1, len(curve_valacc[mode]) + 1)\n            plt.plot(epochs, curve_valacc[mode], label=mode, color=colors.get(mode), linewidth=1)\n    plt.title(\"Validation Accuracy Comparison (All Models)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.xticks(range(1, max_epochs_val + 1))\n    plt.legend()\n    plt.grid(True, linestyle='-', color='lightgray', linewidth=0.8, alpha=0.7)\n    val_acc_path = '/kaggle/working/plots/val_acc_comparison.pdf'\n    plt.savefig(val_acc_path, bbox_inches='tight')\n    plt.show()\n    print(f\"Saved: {val_acc_path}\")\n\n    # List saved model files (if any)\n    if os.path.exists('/kaggle/working/models'):\n        saved_models = [f for f in os.listdir('/kaggle/working/models') if f.endswith('.pth')]\n    else:\n        saved_models = []\n    print(f\"Saved model files: {saved_models}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}