{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====== Testing Code ======\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\n\n# ====== Test configuration ======\nclass TestConfig:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    LFW_PATH = \"/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled\"\n    BATCH_SIZE = 32\n    FEATURE_DIM = 512\n\ntest_config = TestConfig()\n\n# ====== ArcFace model definition consistent with training ======\nclass ArcFaceModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.feature_extractor = nn.Sequential(*list(self.backbone.children())[:-1])\n        self.feature_dim = 512\n        self.fc = nn.Linear(self.feature_dim, num_classes, bias=False)\n        nn.init.normal_(self.fc.weight, std=0.01)\n        self.dropout = nn.Dropout(0.2)\n\n    def get_features(self, x):\n        \"\"\"Safely obtain raw features (without dropout)\"\"\"\n        feat = self.feature_extractor(x)\n        feat = feat.view(feat.size(0), -1)\n        return feat\n\n    def forward(self, x, margin=None):\n        \"\"\"Forward pass identical to training code\"\"\"\n        feat = self.feature_extractor(x)\n        feat = feat.view(feat.size(0), -1)\n        feat = self.dropout(feat)  # used in training, disabled in eval()\n        feat_norm = F.normalize(feat, p=2, dim=1)\n\n        weight_norm = F.normalize(self.fc.weight, p=2, dim=1)\n        cosine = torch.matmul(feat_norm, weight_norm.t())\n\n        # per-sample margin support\n        if margin is None:\n            margin_tensor = 0.0\n        else:\n            if isinstance(margin, (float, int)):\n                margin_tensor = float(margin)\n            elif isinstance(margin, torch.Tensor):\n                if margin.dim() == 1 and margin.size(0) == cosine.size(0):\n                    margin_tensor = margin.view(-1, 1).to(cosine.device)\n                else:\n                    margin_tensor = float(margin.mean().item())\n            else:\n                margin_tensor = float(margin)\n\n        cosine = cosine - margin_tensor\n        output = cosine * 30.0  # use the scale factor from training\n        return feat, output\n\n# ====== LFW test dataset ======\nclass LFWDataset(Dataset):\n    def __init__(self, lfw_path, transform=None, num_pairs=600):\n        self.transform = transform\n        self.lfw_path = lfw_path\n        self.pairs = self._build_pairs_from_lfw(num_pairs)\n        \n    def _build_pairs_from_lfw(self, num_pairs):\n        \"\"\"Build pairs directly from the LFW dataset\"\"\"\n        print(f\"Building test pairs from LFW dataset: {self.lfw_path}\")\n        \n        if not os.path.exists(self.lfw_path):\n            print(f\"ERROR: LFW path does not exist: {self.lfw_path}\")\n            return []\n        \n        person_folders = [f for f in os.listdir(self.lfw_path) \n                         if os.path.isdir(os.path.join(self.lfw_path, f))]\n        \n        print(f\"Found {len(person_folders)} person folders\")\n        \n        if len(person_folders) == 0:\n            print(\"ERROR: No person folders found\")\n            return []\n        \n        person_images = {}\n        valid_persons = []\n        \n        for person in person_folders:\n            person_path = os.path.join(self.lfw_path, person)\n            images = [f for f in os.listdir(person_path) \n                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            if images:\n                person_images[person] = images\n                valid_persons.append(person)\n        \n        print(f\"Valid persons: {len(valid_persons)}\")\n        \n        if len(valid_persons) < 2:\n            print(\"ERROR: Not enough valid persons to build pairs\")\n            return []\n        \n        pairs = []\n        same_person_count = num_pairs // 2\n        diff_person_count = num_pairs - same_person_count\n        \n        same_count = 0\n        persons_with_multiple = [p for p in valid_persons if len(person_images[p]) >= 2]\n        \n        print(f\"Persons with multiple images: {len(persons_with_multiple)}\")\n        \n        for person in persons_with_multiple:\n            if same_count >= same_person_count:\n                break\n            images = person_images[person]\n            img1, img2 = random.sample(images, 2)\n            img1_path = os.path.join(self.lfw_path, person, img1)\n            img2_path = os.path.join(self.lfw_path, person, img2)\n            pairs.append((img1_path, img2_path, 1))\n            same_count += 1\n        \n        diff_count = 0\n        while diff_count < diff_person_count and len(valid_persons) >= 2:\n            person1, person2 = random.sample(valid_persons, 2)\n            images1 = person_images[person1]\n            images2 = person_images[person2]\n            \n            if images1 and images2:\n                img1 = random.choice(images1)\n                img2 = random.choice(images2)\n                img1_path = os.path.join(self.lfw_path, person1, img1)\n                img2_path = os.path.join(self.lfw_path, person2, img2)\n                pairs.append((img1_path, img2_path, 0))\n                diff_count += 1\n        \n        print(f\"Built {len(pairs)} test pairs (same: {same_count}, different: {diff_count})\")\n        \n        if pairs:\n            print(\"\\nExample pairs:\")\n            for i in range(min(3, len(pairs))):\n                path1, path2, label = pairs[i]\n                name1 = os.path.basename(os.path.dirname(path1))\n                name2 = os.path.basename(os.path.dirname(path2))\n                img1 = os.path.basename(path1)\n                img2 = os.path.basename(path2)\n                relation = \"same person\" if label == 1 else \"different person\"\n                print(f\"  {i+1}. {name1}/{img1} vs {name2}/{img2} - {relation}\")\n        \n        return pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img1_path, img2_path, label = self.pairs[idx]\n        \n        try:\n            img1 = Image.open(img1_path).convert('RGB')\n            img2 = Image.open(img2_path).convert('RGB')\n        except Exception as e:\n            print(f\"Failed to load image: {e}\")\n            img1 = Image.new('RGB', (112, 112), color='gray')\n            img2 = Image.new('RGB', (112, 112), color='gray')\n        \n        if self.transform:\n            img1 = self.transform(img1)\n            img2 = self.transform(img2)\n            \n        return img1, img2, torch.tensor(label, dtype=torch.float32)\n\n# ====== Improved model loading function ======\ndef load_trained_model(model_path):\n    \"\"\"Load a trained model - automatically handle class count mismatch\"\"\"\n    try:\n        # Load state dict first to detect class count\n        state_dict = torch.load(model_path, map_location=test_config.device)\n        \n        # Handle different storage formats\n        if isinstance(state_dict, dict):\n            # If dict, check if it includes 'state_dict' and other info\n            if 'state_dict' in state_dict:\n                # New format: contains state_dict and metadata\n                actual_state_dict = state_dict['state_dict']\n                if 'num_classes' in state_dict:\n                    num_classes = state_dict['num_classes']\n                    print(f\"Loaded number of classes from metadata: {num_classes}\")\n                else:\n                    # Infer num_classes from fc.weight\n                    num_classes = actual_state_dict['fc.weight'].shape[0]\n                    print(f\"Inferred number of classes from fc.weight: {num_classes}\")\n            else:\n                # Old format: only state dict\n                actual_state_dict = state_dict\n                num_classes = actual_state_dict['fc.weight'].shape[0]\n                print(f\"Inferred number of classes from fc.weight: {num_classes}\")\n        else:\n            # If it's directly a state dict\n            actual_state_dict = state_dict\n            num_classes = actual_state_dict['fc.weight'].shape[0]\n            print(f\"Inferred number of classes from fc.weight: {num_classes}\")\n        \n        # Create model\n        model = ArcFaceModel(num_classes=num_classes)\n        \n        # Simple key processing: remove 'module.' prefix\n        new_state_dict = {}\n        for k, v in actual_state_dict.items():\n            if k.startswith('module.'):\n                new_k = k[7:]\n            else:\n                new_k = k\n            new_state_dict[new_k] = v\n        \n        # Try strict loading\n        try:\n            model.load_state_dict(new_state_dict, strict=True)\n            print(\"Strict load succeeded\")\n        except RuntimeError as e:\n            if \"size mismatch\" in str(e):\n                print(\"Size mismatch detected, attempting flexible loading...\")\n                # Flexible loading: only load matching layers\n                model_state_dict = model.state_dict()\n                filtered_state_dict = {}\n                \n                for key, value in new_state_dict.items():\n                    if key in model_state_dict:\n                        if model_state_dict[key].shape == value.shape:\n                            filtered_state_dict[key] = value\n                        else:\n                            print(f\"  Skipping mismatched layer: {key} (expected: {model_state_dict[key].shape}, actual: {value.shape})\")\n                    else:\n                        print(f\"  Skipping non-existing layer: {key}\")\n                \n                # Load filtered state dict\n                model.load_state_dict(filtered_state_dict, strict=False)\n                print(f\"Flexible load complete, loaded {len(filtered_state_dict)}/{len(new_state_dict)} layers\")\n            else:\n                raise e\n        \n        model.to(test_config.device)\n        model.eval()  # important: disable dropout and set BN to eval\n        \n        # Verify model loaded correctly\n        with torch.no_grad():\n            test_input = torch.randn(1, 3, 112, 112).to(test_config.device)\n            features, outputs = model(test_input)\n            print(f\"Model verification: feature shape {features.shape}, output shape {outputs.shape}\")\n            \n        return model\n        \n    except Exception as e:\n        print(f\"Failed to load model {model_path}: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n# ====== Feature extraction ======\ndef extract_features(model, images):\n    \"\"\"Extract image features - use get_features for consistency\"\"\"\n    with torch.no_grad():\n        # Use get_features instead of forward to ensure features used for margin calculation are consistent\n        features = model.get_features(images)\n        features = F.normalize(features, p=2, dim=1)\n    return features\n\n# ====== Compute accuracy and ROC curve ======\ndef evaluate_model(model, test_loader, model_name):\n    \"\"\"Evaluate model performance on the test set\"\"\"\n    if len(test_loader.dataset) == 0:\n        print(f\"ERROR: Test dataset is empty, cannot evaluate {model_name}\")\n        return 0, 0, 0, [], []\n    \n    similarities = []\n    labels = []\n    \n    print(f\"Evaluating model {model_name}...\")\n    \n    with torch.no_grad():\n        for img1, img2, label in tqdm(test_loader, desc=f\"Testing {model_name}\"):\n            img1, img2 = img1.to(test_config.device), img2.to(test_config.device)\n            \n            # Use consistent feature extraction method\n            feat1 = extract_features(model, img1)\n            feat2 = extract_features(model, img2)\n            \n            similarity = F.cosine_similarity(feat1, feat2)\n            \n            similarities.extend(similarity.cpu().numpy())\n            labels.extend(label.numpy())\n    \n    similarities = np.array(similarities)\n    labels = np.array(labels)\n    \n    # Compute ROC and AUC\n    fpr, tpr, thresholds = roc_curve(labels, similarities)\n    roc_auc = auc(fpr, tpr)\n    \n    gmeans = np.sqrt(tpr * (1-fpr))\n    ix = np.argmax(gmeans)\n    best_threshold = thresholds[ix]\n    \n    predictions = (similarities >= best_threshold).astype(int)\n    accuracy = np.mean(predictions == labels)\n    \n    print(f\"{model_name} results:\")\n    print(f\"   Accuracy: {accuracy:.4f}\")\n    print(f\"   AUC: {roc_auc:.4f}\")\n    print(f\"   Best threshold: {best_threshold:.4f}\")\n    print(f\"   Similarity range: [{similarities.min():.3f}, {similarities.max():.3f}]\")\n    \n    return accuracy, roc_auc, best_threshold, fpr, tpr\n\n# ====== Plot ROC curves ======\ndef plot_roc_curves(results, save_path='/kaggle/working/roc_curves.pdf'):\n    \"\"\"Plot ROC curves for all models\"\"\"\n    plt.figure(figsize=(10, 8))\n    \n    colors = ['red', 'blue', 'green', 'orange']\n    for i, (model_name, (accuracy, roc_auc, _, fpr, tpr)) in enumerate(results.items()):\n        plt.plot(fpr, tpr, color=colors[i], lw=2, \n                label=f'{model_name} (AUC = {roc_auc:.3f}, Acc = {accuracy:.3f})')\n    \n\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('LFW - ROC')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    \n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n\n# ====== Analyze results ======\ndef analyze_results(results):\n    \"\"\"Analyze test results\"\"\"\n    print(\"\\nResults Analysis:\")\n    print(\"=\" * 30)\n    \n    if len(results) < 2:\n        print(\"At least two model results are required for comparison\")\n        return\n    \n    sorted_results = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n    \n    print(\"Model ranking (by accuracy):\")\n    for i, (model_name, (accuracy, roc_auc, threshold, _, _)) in enumerate(sorted_results):\n        print(f\"  {i+1}. {model_name}: {accuracy:.4f}\")\n    \n    best_acc = sorted_results[0][1][0]\n    worst_acc = sorted_results[-1][1][0]\n    improvement = best_acc - worst_acc\n    \n    print(f\"\\nBest model accuracy is higher than worst model by: {improvement:.4f} ({improvement*100:.2f}%)\")\n    \n    if \"fixed_margin\" in results:\n        fixed_acc = results[\"fixed_margin\"][0]\n        for model_name, (accuracy, _, _, _, _) in results.items():\n            if model_name != \"fixed_margin\":\n                diff = accuracy - fixed_acc\n                if diff > 0:\n                    print(f\"{model_name} is better than fixed_margin: +{diff:.4f} (+{diff*100:.2f}%)\")\n                elif diff < 0:\n                    print(f\"{model_name} is worse than fixed_margin: {diff:.4f} ({diff*100:.2f}%)\")\n                else:\n                    print(f\"{model_name} is equal to fixed_margin\")\n\n# ====== Main test function ======\ndef test_models_on_lfw():\n    \"\"\"Test all models on the LFW dataset\"\"\"\n    \n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.CenterCrop((112, 112)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    print(\"Creating LFW test dataset...\")\n    test_dataset = LFWDataset(\n        lfw_path=test_config.LFW_PATH,\n        transform=transform,\n        num_pairs=600\n    )\n    \n    if len(test_dataset) == 0:\n        print(\"ERROR: Failed to create LFW test dataset\")\n        return\n    \n    test_loader = DataLoader(test_dataset, batch_size=test_config.BATCH_SIZE, shuffle=False)\n    \n    model_paths = {\n        \"fixed_margin\": \"/kaggle/working/models/best_model_fixed_margin.pth\",\n        \"quality_adaptive\": \"/kaggle/working/models/best_model_quality_adaptive.pth\", \n        \"confidence_adaptive\": \"/kaggle/working/models/best_model_confidence_adaptive.pth\",\n        \"easy_hard_norm\": \"/kaggle/working/models/best_model_easy_hard_norm.pth\"\n    }\n\n    results = {}\n\n    for model_name, model_path in model_paths.items():\n        if not os.path.exists(model_path):\n           print(f\"WARNING: Model not found: {model_path}\")\n           continue\n\n        print(f\"\\n{'='*50}\")\n        print(f\"Loading and testing model: {model_name}...\")\n        print(f\"{'='*50}\")\n\n        model = load_trained_model(model_path)\n        if model is None:\n            continue\n\n        # LFW evaluation uses cosine similarity without training-time margin by default\n        accuracy, roc_auc, threshold, fpr, tpr = evaluate_model(model, test_loader, model_name)\n        results[model_name] = (accuracy, roc_auc, threshold, fpr, tpr)\n\n        if torch.cuda.is_available():\n           torch.cuda.empty_cache()\n\n    \n    if results:\n        plot_roc_curves(results)\n        \n        print(\"\\nLFW final results:\")\n        print(\"=\" * 50)\n        for model_name, (accuracy, roc_auc, threshold, _, _) in results.items():\n            print(f\"{model_name:20} | Accuracy: {accuracy:.4f} | AUC: {roc_auc:.4f} | Threshold: {threshold:.4f}\")\n        \n        best_model = max(results.items(), key=lambda x: x[1][0])\n        print(f\"\\nBest model: {best_model[0]} (Accuracy: {best_model[1][0]:.4f})\")\n        \n        analyze_results(results)\n        \n        with open('/kaggle/working/lfw_test_results.txt', 'w') as f:\n            f.write(\"LFW Test Results\\n\")\n            f.write(\"=\" * 50 + \"\\n\")\n            for model_name, (accuracy, roc_auc, threshold, _, _) in results.items():\n                f.write(f\"{model_name:20} | Accuracy: {accuracy:.4f} | AUC: {roc_auc:.4f} | Threshold: {threshold:.4f}\\n\")\n            f.write(f\"\\nBest model: {best_model[0]} (Accuracy: {best_model[1][0]:.4f})\\n\")\n        \n        print(\"Results saved to: /kaggle/working/lfw_test_results.txt\")\n    else:\n        print(\"ERROR: No models were successfully tested\")\n\n# ====== Run tests ======\nif __name__ == \"__main__\":\n    print(\"Starting LFW evaluation...\")\n    print(f\"Using LFW path: {test_config.LFW_PATH}\")\n    test_models_on_lfw()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}